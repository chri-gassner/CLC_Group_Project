#!/usr/bin/env python3
"""
demo_tabular_panel.py

MediaPipe Tasks PoseLandmarker (.task) + tabular window classifier inference
with a clean UI: video on the left, info panel on the right (no text overlay).

Behavior:
- webcam/video
- downsample to FPS_TARGET
- sliding window WIN_FRAMES with STEP_FRAMES
- extract pose -> engineered per-frame features -> aggregate over window
- query trained TABULAR model(s) and show skeleton + top1/top3 in side panel

You MUST set MODEL_PATH to your downloaded .task file (e.g. pose_landmarker_heavy.task).
"""

import json
from collections import deque
from pathlib import Path

import cv2
import numpy as np
import joblib
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision


# =========================
# CONFIG (edit here)
# =========================
# --- inference model (tabular) ---
MODEL_DIR = Path("../output/models_output/mediapipe_dataset/")

MODEL_FILES = [
    "mediapipe_dataset_classifier_knn_best.pkl",
    "mediapipe_dataset_classifier_mlp_neuralnet_best.pkl",
]

FEATURE_NAMES_PKL = "feature_names.pkl"
LABEL_ENCODER_PKL = "label_encoder.pkl"
CLASS_NAMES_JSON = "class_names.json"  # fallback

# --- video source ---
VIDEO_SOURCE = 0  # 0 = webcam, or "path/to/video.mp4"

# --- MediaPipe Tasks PoseLandmarker (.task) ---
MODEL_PATH = "../extract_pose_mediapipe/pose_landmarker_heavy.task"

# --- sampling / windowing ---
FPS_TARGET = 15
WIN_SEC = 2.0
STEP_SEC = 1.0
WIN_FRAMES = int(WIN_SEC * FPS_TARGET)
STEP_FRAMES = int(STEP_SEC * FPS_TARGET)

# --- feature builder thresholds (must match build_feature_dataset.py) ---
VIS_THRESH = 0.5
MIN_VALID_FRAC = 0.6
NAN_TO_ZERO = True

# --- drawing ---
DRAW_MIN_VIS = 0.2  # only draw points/edges above this visibility

# --- UI panel ---
PANEL_W = 460
PADDING = 6  # border around video


# =========================
# Pose connections for drawing (from solutions)
# =========================
try:
    from mediapipe.python.solutions.pose import POSE_CONNECTIONS as MP_POSE_CONNECTIONS
except Exception:
    print("[WARN] using hardcoded MP_POSE_CONNECTIONS")
    MP_POSE_CONNECTIONS = [
        (0,1),(1,2),(2,3),(3,7),
        (0,4),(4,5),(5,6),(6,8),
        (9,10),
        (11,12),
        (11,13),(13,15),(15,17),(15,19),(15,21),
        (12,14),(14,16),(16,18),(16,20),(16,22),
        (11,23),(12,24),
        (23,24),
        (23,25),(25,27),(27,29),(29,31),
        (24,26),(26,28),(28,30),(30,32),
        (27,31),(28,32),
    ]


# =========================
# Feature logic (must match build_feature_dataset.py)
# =========================
L_SHOULDER, R_SHOULDER = 11, 12
L_ELBOW, R_ELBOW = 13, 14
L_WRIST, R_WRIST = 15, 16
L_HIP, R_HIP = 23, 24
L_KNEE, R_KNEE = 25, 26
L_ANKLE, R_ANKLE = 27, 28

KEY_IDX = np.array([L_SHOULDER, R_SHOULDER, L_HIP, R_HIP, L_KNEE, R_KNEE, L_ELBOW, R_ELBOW])


def _angle(a, b, c):
    ba = a - b
    bc = c - b
    nba = np.linalg.norm(ba)
    nbc = np.linalg.norm(bc)
    if nba == 0 or nbc == 0 or np.any(np.isnan([nba, nbc])):
        return np.nan
    cosang = np.dot(ba, bc) / (nba * nbc)
    cosang = np.clip(cosang, -1.0, 1.0)
    return float(np.degrees(np.arccos(cosang)))


def _dist(a, b):
    d = np.linalg.norm(a - b)
    return float(d) if not np.isnan(d) else np.nan


def per_frame_features_from_window(win_arr):
    """
    win_arr: (T,33,4) float32 with possible NaNs
    returns feats dict[name]->(T,) and valid mask
    """
    xyz = win_arr[:, :, :3]
    vis = win_arr[:, :, 3]
    T = win_arr.shape[0]

    mean_vis = np.nanmean(vis[:, KEY_IDX], axis=1)
    valid = mean_vis >= VIS_THRESH

    shoulder_mid = 0.5 * (xyz[:, L_SHOULDER] + xyz[:, R_SHOULDER])
    hip_mid = 0.5 * (xyz[:, L_HIP] + xyz[:, R_HIP])

    shoulder_width = np.array([_dist(xyz[t, L_SHOULDER], xyz[t, R_SHOULDER]) for t in range(T)], dtype=np.float32)
    hip_width = np.array([_dist(xyz[t, L_HIP], xyz[t, R_HIP]) for t in range(T)], dtype=np.float32)
    torso_len = np.array([_dist(shoulder_mid[t], hip_mid[t]) for t in range(T)], dtype=np.float32)

    scale = torso_len.astype(np.float32)
    scale = np.where(scale < 1e-6, np.nan, scale)

    def norm_dist(p, q):
        d = np.array([_dist(p[t], q[t]) for t in range(T)], dtype=np.float32)
        return d / scale

    elbow_l = np.array([_angle(xyz[t, L_SHOULDER], xyz[t, L_ELBOW], xyz[t, L_WRIST]) for t in range(T)], dtype=np.float32)
    elbow_r = np.array([_angle(xyz[t, R_SHOULDER], xyz[t, R_ELBOW], xyz[t, R_WRIST]) for t in range(T)], dtype=np.float32)

    knee_l = np.array([_angle(xyz[t, L_HIP], xyz[t, L_KNEE], xyz[t, L_ANKLE]) for t in range(T)], dtype=np.float32)
    knee_r = np.array([_angle(xyz[t, R_HIP], xyz[t, R_KNEE], xyz[t, R_ANKLE]) for t in range(T)], dtype=np.float32)

    hip_l = np.array([_angle(xyz[t, L_SHOULDER], xyz[t, L_HIP], xyz[t, L_KNEE]) for t in range(T)], dtype=np.float32)
    hip_r = np.array([_angle(xyz[t, R_SHOULDER], xyz[t, R_HIP], xyz[t, R_KNEE]) for t in range(T)], dtype=np.float32)

    shoulder_l = np.array([_angle(xyz[t, L_ELBOW], xyz[t, L_SHOULDER], xyz[t, L_HIP]) for t in range(T)], dtype=np.float32)
    shoulder_r = np.array([_angle(xyz[t, R_ELBOW], xyz[t, R_SHOULDER], xyz[t, R_HIP]) for t in range(T)], dtype=np.float32)

    wrist2hip_l = norm_dist(xyz[:, L_WRIST], hip_mid)
    wrist2hip_r = norm_dist(xyz[:, R_WRIST], hip_mid)
    ankle2hip_l = norm_dist(xyz[:, L_ANKLE], hip_mid)
    ankle2hip_r = norm_dist(xyz[:, R_ANKLE], hip_mid)

    feats = {
        "mean_vis": mean_vis.astype(np.float32),
        "shoulder_width": shoulder_width,
        "hip_width": hip_width,
        "torso_len": torso_len,

        "elbow_l": elbow_l, "elbow_r": elbow_r,
        "knee_l": knee_l, "knee_r": knee_r,
        "hip_l": hip_l, "hip_r": hip_r,
        "shoulder_l": shoulder_l, "shoulder_r": shoulder_r,

        "wrist2hip_l": wrist2hip_l.astype(np.float32),
        "wrist2hip_r": wrist2hip_r.astype(np.float32),
        "ankle2hip_l": ankle2hip_l.astype(np.float32),
        "ankle2hip_r": ankle2hip_r.astype(np.float32),
    }

    # invalid frames -> NaN
    for k in list(feats.keys()):
        v = feats[k].copy()
        v[~valid] = np.nan
        feats[k] = v

    return feats, valid


def agg_stats(vec):
    out = {
        "mean": float(np.nanmean(vec)),
        "std": float(np.nanstd(vec)),
        "min": float(np.nanmin(vec)),
        "max": float(np.nanmax(vec)),
    }
    if np.sum(~np.isnan(vec)) >= 2:
        dv = np.diff(vec)
        out["madiff"] = float(np.nanmean(np.abs(dv)))
    else:
        out["madiff"] = np.nan
    return out


def build_window_feature_row(win_arr):
    feats, valid = per_frame_features_from_window(win_arr)
    valid_frac = float(np.mean(valid))
    if valid_frac < MIN_VALID_FRAC:
        return None, valid_frac

    row = {"valid_frac": valid_frac}
    for name, series in feats.items():
        if np.all(np.isnan(series)):
            continue
        a = agg_stats(series)
        for stat, val in a.items():
            row[f"{name}__{stat}"] = val
    return row, valid_frac


# =========================
# Model loading + inference
# =========================
def load_model(model_dir: Path, model_file: str | None):
    if model_file:
        p = model_dir / model_file
        if not p.exists():
            raise FileNotFoundError(f"MODEL_FILE not found: {p}")
        return joblib.load(p), p.name

    candidates = sorted(model_dir.glob("*_best.pkl"))
    if not candidates:
        raise FileNotFoundError(f"No *_best.pkl found in {model_dir}")
    c = candidates[0]
    return joblib.load(c), c.name


def load_class_names(model_dir: Path):
    p = model_dir / LABEL_ENCODER_PKL
    if p.exists():
        le = joblib.load(p)
        return list(le.classes_), le

    p2 = model_dir / CLASS_NAMES_JSON
    if p2.exists():
        with open(p2, "r") as f:
            names = json.load(f)
        return names, None

    raise FileNotFoundError(f"Missing {LABEL_ENCODER_PKL} or {CLASS_NAMES_JSON} in {model_dir}")


def ensure_feature_order(row_dict, feature_names):
    x = np.zeros((1, len(feature_names)), dtype=np.float32)
    for j, fn in enumerate(feature_names):
        val = row_dict.get(fn, 0.0)
        if val is None or (isinstance(val, float) and np.isnan(val)):
            val = 0.0
        x[0, j] = float(val)
    return x


def predict_tabular(model, x_row, class_names):
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(x_row)[0]
        pred_id = int(np.argmax(proba))
        conf = float(proba[pred_id])
        topk_ids = np.argsort(proba)[::-1][:3]
        topk = [(class_names[int(i)], float(proba[int(i)])) for i in topk_ids]
        return class_names[pred_id], conf, topk
    pred_id = int(model.predict(x_row)[0])
    return class_names[pred_id], float("nan"), [(class_names[pred_id], float("nan"))]


# =========================
# MediaPipe Tasks PoseLandmarker extraction (VIDEO mode)
# =========================
def _lm_list_to_arr5(lm_list):
    """
    lm_list: list of 33 landmarks (NormalizedLandmark or Landmark)
    returns: (33,5) x,y,z,visibility,presence (may contain NaNs)
    """
    arr = np.zeros((33, 5), dtype=np.float32)
    for i, p in enumerate(lm_list):
        arr[i, 0] = getattr(p, "x", np.nan)
        arr[i, 1] = getattr(p, "y", np.nan)
        arr[i, 2] = getattr(p, "z", np.nan)
        arr[i, 3] = getattr(p, "visibility", np.nan)
        arr[i, 4] = getattr(p, "presence", np.nan)
    return arr


def extract_pose_frame_open(frame_bgr, landmarker, t_ms: int):
    """
    Returns:
      arr4: (33,4) [x,y,z,visibility] or None if no detection
      arr5_img: (33,5) for drawing (optional) or None
    """
    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)

    res = landmarker.detect_for_video(mp_image, t_ms)

    if not res.pose_landmarks:
        return None, None

    lm_img = res.pose_landmarks[0]  # 33 normalized landmarks
    arr5 = _lm_list_to_arr5(lm_img)
    arr4 = arr5[:, :4].astype(np.float32)  # x,y,z,visibility
    return arr4, arr5


# =========================
# Drawing helpers
# =========================
def draw_skeleton(frame_bgr, arr5):
    """
    arr5: (33,5) normalized coords
    """
    if arr5 is None:
        return
    h, w = frame_bgr.shape[:2]

    # draw connections
    for (i, j) in MP_POSE_CONNECTIONS:
        vi = arr5[i, 3]
        vj = arr5[j, 3]
        if np.isfinite(vi) and np.isfinite(vj):
            if vi < DRAW_MIN_VIS or vj < DRAW_MIN_VIS:
                continue
        xi, yi = int(arr5[i, 0] * w), int(arr5[i, 1] * h)
        xj, yj = int(arr5[j, 0] * w), int(arr5[j, 1] * h)
        cv2.line(frame_bgr, (xi, yi), (xj, yj), (0, 255, 0), 2)

    # draw points
    for k in range(33):
        vk = arr5[k, 3]
        if np.isfinite(vk) and vk < DRAW_MIN_VIS:
            continue
        xk, yk = int(arr5[k, 0] * w), int(arr5[k, 1] * h)
        cv2.circle(frame_bgr, (xk, yk), 3, (0, 255, 0), -1)


def wrap_lines(lines, max_chars=46):
    """Simple word-wrap per line (character-based)."""
    out = []
    for line in lines:
        s = str(line)
        while len(s) > max_chars:
            cut = s.rfind(" ", 0, max_chars)
            if cut == -1:
                cut = max_chars
            out.append(s[:cut].rstrip())
            s = s[cut:].lstrip()
        out.append(s)
    return out


def draw_info_panel(
    panel,
    lines,
    x=16,
    y=34,
    dy=30,
    font_scale=0.75,
    thickness=2,
    text_color=(220, 220, 220),
    header_color=(245, 245, 245),
    max_chars=46,
):
    panel[:] = (18, 18, 18)  # dark background

    # header + separator
    header = "Pose + Prediction"
    cv2.putText(
        panel,
        header,
        (x, y),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.9,
        header_color,
        2,
        cv2.LINE_AA,
    )
    cv2.line(panel, (x, y + 10), (panel.shape[1] - x, y + 10), (60, 60, 60), 1)

    y0 = y + 38
    lines_wrapped = wrap_lines(lines, max_chars=max_chars)

    # clamp to available height
    h = panel.shape[0]
    max_lines = max(1, (h - y0 - 10) // dy)
    lines_wrapped = lines_wrapped[:max_lines]

    for i, line in enumerate(lines_wrapped):
        cv2.putText(
            panel,
            line,
            (x, y0 + i * dy),
            cv2.FONT_HERSHEY_SIMPLEX,
            font_scale,
            text_color,
            thickness,
            cv2.LINE_AA,
        )


def compose_with_panel(frame_bgr, lines, panel_w=PANEL_W, padding=PADDING):
    """
    Returns a new image: [video(with border) | info-panel]
    """
    frame = cv2.copyMakeBorder(
        frame_bgr,
        padding, padding, padding, padding,
        borderType=cv2.BORDER_CONSTANT,
        value=(0, 0, 0)
    )
    h2, _ = frame.shape[:2]
    panel = np.zeros((h2, panel_w, 3), dtype=np.uint8)

    draw_info_panel(panel, lines)
    return np.hstack([frame, panel])


# =========================
# Main
# =========================
def main():
    if not MODEL_DIR.exists():
        raise FileNotFoundError(f"MODEL_DIR not found: {MODEL_DIR}")

    models = []
    for mf in MODEL_FILES:
        m, mn = load_model(MODEL_DIR, mf)
        models.append((m, mn))
    class_names, _ = load_class_names(MODEL_DIR)

    feature_names_path = MODEL_DIR / FEATURE_NAMES_PKL
    if not feature_names_path.exists():
        raise FileNotFoundError(f"Missing {FEATURE_NAMES_PKL} in {MODEL_DIR}")
    feature_names = joblib.load(feature_names_path)

    if not Path(MODEL_PATH).exists():
        raise FileNotFoundError(f"PoseLandmarker .task not found: {MODEL_PATH}")

    print(f"[INFO] Loaded models: {[model_name for _, model_name in models]}")
    print(f"[INFO] Features expected: {len(feature_names)}")
    print(f"[INFO] Classes: {len(class_names)}")
    print(f"[INFO] PoseLandmarker task: {MODEL_PATH}")

    cap = cv2.VideoCapture(VIDEO_SOURCE)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open VIDEO_SOURCE={VIDEO_SOURCE}")

    fps_in = cap.get(cv2.CAP_PROP_FPS) or 30.0
    stride = max(1, int(round(fps_in / FPS_TARGET)))
    print(f"[INFO] fps_in={fps_in:.2f} -> FPS_TARGET={FPS_TARGET} stride={stride}")
    print(f"[INFO] WIN_FRAMES={WIN_FRAMES} STEP_FRAMES={STEP_FRAMES}")

    # PoseLandmarker (VIDEO mode)
    base_options = python.BaseOptions(model_asset_path=MODEL_PATH)
    options = vision.PoseLandmarkerOptions(
        base_options=base_options,
        running_mode=vision.RunningMode.VIDEO,
        output_segmentation_masks=False,
    )
    landmarker = vision.PoseLandmarker.create_from_options(options)

    window = deque(maxlen=WIN_FRAMES)

    # We maintain our own "processed frame index" and timestamps
    # timestamp must be monotonically increasing in ms for VIDEO mode.
    i_raw = 0          # raw frame counter
    i_used = 0         # processed frames counter (after stride)
    last_pred_used = -10_000
    last_pred = ([], 0.0)  # (preds_list, valid_frac)

    try:
        while True:
            ok, frame = cap.read()
            if not ok:
                break

            # downsample
            if i_raw % stride != 0:
                i_raw += 1
                continue

            # timestamp (ms) based on original fps and raw frame index
            t_ms = int((i_raw / fps_in) * 1000)

            arr4, arr5 = extract_pose_frame_open(frame, landmarker, t_ms)

            # draw skeleton
            if arr5 is not None:
                draw_skeleton(frame, arr5)

            if arr4 is None:
                arr4 = np.full((33, 4), np.nan, dtype=np.float32)

            window.append(arr4)

            # predict on step
            if len(window) == WIN_FRAMES:
                if (i_used - last_pred_used) >= STEP_FRAMES:
                    win_arr = np.stack(list(window), axis=0)  # (WIN,33,4)

                    if NAN_TO_ZERO:
                        win_arr = np.nan_to_num(win_arr, nan=0.0).astype(np.float32)

                    row_dict, valid_frac = build_window_feature_row(win_arr)
                    if row_dict is not None:
                        x_row = ensure_feature_order(row_dict, feature_names)

                        preds = []
                        for m, mn in models:
                            pl, cf, tk = predict_tabular(m, x_row, class_names)
                            preds.append((mn, pl, cf, tk))

                        last_pred = (preds, valid_frac)
                    else:
                        preds, _ = last_pred
                        last_pred = (preds, valid_frac)

                    last_pred_used = i_used

            preds, valid_frac = last_pred

            # build panel text
            lines = [
                f"valid_frac={valid_frac:.2f}",
                f"FPS_TARGET={FPS_TARGET}  stride={stride}",
                f"WIN={WIN_FRAMES} ({WIN_SEC:.1f}s)  STEP={STEP_FRAMES} ({STEP_SEC:.1f}s)",
                "",
            ]

            for mn, pl, cf, tk in preds[:2]:
                if np.isfinite(cf):
                    lines.append(f"{mn}")
                    lines.append(f"  pred: {pl}   conf={cf:.3f}")
                    lines.append("  Top3: " + " | ".join([f"{lbl}:{p:.2f}" for lbl, p in tk]))
                    lines.append("")
                else:
                    lines.append(f"{mn}: {pl}")
                    lines.append("")

            ui = compose_with_panel(frame, lines, panel_w=PANEL_W, padding=PADDING)

            cv2.imshow("demo_tabular (tasks) - Pose + Prediction", ui)
            key = cv2.waitKey(1) & 0xFF
            if key in (27, ord("q")):
                break

            i_raw += 1
            i_used += 1

    finally:
        cap.release()
        cv2.destroyAllWindows()
        landmarker.close()


if __name__ == "__main__":
    main()
